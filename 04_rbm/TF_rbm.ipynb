{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Restricted Boltzmann Machine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These are shallow neural networks that learns to reconstruct data by an unsupervised manner. The first layer is called <b>Visible Layer</b> and the second layer is called <b>Invisible Layer</b>. Its called restricted because the neurons in the same layer are not connected to each other. RBM is a generative model. A generative model specify a probability distribution over a dataset of input vectors. We can do both supervise and unsupervised tasks with generative models.\n",
    "    \n",
    "- In unsupervised task we design the model to find P(x), where P is the probability given x as an input.\n",
    "- In supervised task we desing the model to find P(x|y), that is probability of x given y(label of x)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing stuff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "we will be import python script that will help us in processing the outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "from PIL import Image\n",
    "from utils import tile_raster_images\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The RMB has 2 layers (visible and hidden). Here we have 7 neurons for our visible layer and 2 neurons for our invisible layer. Each neuron in a layer will have its bias."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "v_bias = tf.placeholder('float', [7]) # visible layer bias\n",
    "h_bias = tf.placeholder('float', [2]) # hidden layer bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here the weights matrix between the visible and hidden layer will be of 7x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.constant(np.random.normal(loc=0.0, scale=1.0,\n",
    "                                 size=(7,2)).astype(np.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBM has 2 phases: \n",
    "- Forward pass\n",
    "- Backword pass or Reconstruction\n",
    " \n",
    "1) Forward Pass: <br>\n",
    "In forward pass the model takes one input say X through all the visible nodes, and pass it to the hidden nodes. In the hidden node the input X is multiplied by $W_{ij}$ and then added to h_bias. The result is then fed into a sigmoid function, which gives the output, that is $P({h_j})$, where j is the unit number.\n",
    " \n",
    "Here $P({h_j})$ represents the probabilities of the hidden units. And all values together its called the probability distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INPUT:  [[1. 0. 0. 1. 0. 0. 0.]]\n",
      "h_bias:  [0.1 0.1]\n",
      "W:  [[-1.4318179  -1.6963902 ]\n",
      " [-0.22592606 -0.36571947]\n",
      " [-1.2760918   1.0116413 ]\n",
      " [-0.14385842 -0.3269168 ]\n",
      " [-1.6623821  -1.1617765 ]\n",
      " [ 0.05079454 -0.9133033 ]\n",
      " [ 1.6075933  -0.31550974]]\n",
      "P(h|v):  [[0.18608138 0.12749326]]\n",
      "h0 states:  [[0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "sess = tf.Session()\n",
    "X = tf.constant([[1.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0]])\n",
    "v_state = X\n",
    "print(\"INPUT: \", sess.run(v_state))\n",
    "\n",
    "h_bias = tf.constant([0.1,0.1])\n",
    "print(\"h_bias: \", sess.run(h_bias))\n",
    "print(\"W: \", sess.run(W))\n",
    "\n",
    "# hidden layer output\n",
    "h_prob = tf.nn.sigmoid(tf.matmul(v_state, W) + h_bias) \n",
    "print(\"P(h|v): \", sess.run(h_prob))\n",
    "\n",
    "# Drawing samples from the distribution\n",
    "h_state = tf.nn.relu(tf.sign(h_prob - tf.random_uniform(tf.shape(h_prob)))) # states\n",
    "print(\"h0 states: \", sess.run(h_state))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2) Backword Pass or Reconstruction: <br>\n",
    "\n",
    "Now the hidden layer will act as the input to the model. Means, h will become the input in backward pass, with the same weight matrix and bias the produced output will try to reconstruct the original input."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bias:  [0.1 0.2 0.1 0.1 0.1 0.2 0.1]\n",
      "P(vi|h):  [[0.5249792  0.54983395 0.5249792  0.5249792  0.5249792  0.54983395\n",
      "  0.5249792 ]]\n",
      "v probability states:  [[1. 0. 1. 1. 1. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "vb = tf.constant([0.1,0.2,0.1,0.1,0.1,0.2,0.1])\n",
    "print(\"bias: \", sess.run(vb))\n",
    "v_prob = sess.run(tf.nn.sigmoid(tf.matmul(h_state, tf.transpose(W)) + vb))\n",
    "print(\"P(vi|h): \", v_prob)\n",
    "v_state = tf.nn.relu(tf.sign(v_prob - tf.random_uniform(tf.shape(v_prob))))\n",
    "print(\"v probability states: \" , sess.run(v_state))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
