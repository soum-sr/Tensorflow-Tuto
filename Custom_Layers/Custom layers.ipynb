{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential()\n",
    "# Densely connected layer with 64 units to the model\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "model.add(layers.Dense(64, activation='relu'))\n",
    "# Softmax layer\n",
    "model.add(layers.Dense(10, activation='softmax'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model subclassing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can build a fully customizable model by sublclassing <b>tf.keras.Model</b> and defining our own forward pass. Here we set the attributes in the $__init__$ method and the forward pass in the call method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(tf.keras.Model):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(MyModel, self).__init__(name='my_model')\n",
    "        self.num_classes = num_classes\n",
    "        # Define the layers\n",
    "        self.dense_1 = layers.Dense(32, activation='relu')\n",
    "        self.dense_2 = layers.Dense(num_classes, activation='sigmoid')\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        # Forward pass\n",
    "        x = self.dense_1(inputs)\n",
    "        return self.dense_2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model class\n",
    "model = MyModel(num_classes=10)\n",
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001), loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W0802 23:31:24.444645 139713071793984 deprecation.py:323] From /home/soumyajit/.local/lib/python3.6/site-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 1s 1ms/sample - loss: 11.5400 - accuracy: 0.0900\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 33us/sample - loss: 11.4897 - accuracy: 0.1020\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 44us/sample - loss: 11.4826 - accuracy: 0.0910\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 39us/sample - loss: 11.4784 - accuracy: 0.0970\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 41us/sample - loss: 11.4757 - accuracy: 0.1020\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f112f6f1c50>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, labels, batch_size=32, epochs=5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create custom layers by sublclassing <b>tf.keras.layers.Layer</b> and<br>\n",
    "- \\__init__ : Optionally define sublayers to be used by this layer\n",
    "- build : Create the weights of the layer. And add weights with the <b>add_weight</b> method\n",
    "- call : Define the forward pass\n",
    "- A layer can be serialized by implementing the get_config method and the from_config class method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = np.random.random((1000, 32))\n",
    "data2 = np.random.random((1000, 32))\n",
    "labels = np.random.random((1000, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = [data, data2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1000, 32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, output_dim, **kwargs):\n",
    "        print(\"We are in __init__\")\n",
    "        self.output_dim = output_dim\n",
    "        super(MyLayer, self).__init__(**kwargs)\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        print(\"We are in  build\")\n",
    "        print(input_shape)\n",
    "        # create a trainable weight variable\n",
    "        self.kernel = self.add_weight(name='kernel',\n",
    "                                     shape=(input_shape[1], self.output_dim),\n",
    "                                     initializer = 'uniform',\n",
    "                                     trainable = True)\n",
    "        #print(\"Kernel: \", self.kernel)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(\"We are in call\")\n",
    "        #input_1, input_2 = inputs\n",
    "        #return tf.add(tf.matmul(input_1, self.kernel), input_2)\n",
    "        return tf.matmul(inputs, self.kernel)\n",
    "    \n",
    "    def get_config(self):\n",
    "        print(\"We are in get_config\")\n",
    "        base_config = super(MyLayer, self).get_config()\n",
    "        base_config['output_dim'] = self.output_dim\n",
    "        return base_config\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in __init__\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.Sequential([MyLayer(10), layers.Activation('softmax')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.RMSprop(0.001),\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We are in  build\n",
      "(None, 32)\n",
      "We are in call\n",
      "Train on 1000 samples\n",
      "Epoch 1/5\n",
      "1000/1000 [==============================] - 0s 216us/sample - loss: 11.6657 - accuracy: 0.1190\n",
      "Epoch 2/5\n",
      "1000/1000 [==============================] - 0s 34us/sample - loss: 11.6615 - accuracy: 0.1230\n",
      "Epoch 3/5\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 11.6599 - accuracy: 0.1200\n",
      "Epoch 4/5\n",
      "1000/1000 [==============================] - 0s 42us/sample - loss: 11.6578 - accuracy: 0.1160\n",
      "Epoch 5/5\n",
      "1000/1000 [==============================] - 0s 36us/sample - loss: 11.6566 - accuracy: 0.1300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f11143c62b0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(data, labels, batch_size=32, epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The layer class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=32, input_dim=32):\n",
    "        super(Linear, self).__init__()\n",
    "        w_init = tf.random_normal_initializer()\n",
    "        self.w = tf.Variable(initial_value=w_init(shape=(input_dim, units),\n",
    "                                              dtype='float32'),trainable=True)\n",
    "        b_init = tf.zeros_initializer()\n",
    "        self.b = tf.Variable(initial_value=b_init(shape=(units,), dtype='float32'), trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(\"I am in call\")\n",
    "        return tf.matmul(inputs, self.w) + self.b\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am in call\n",
      "tf.Tensor(\n",
      "[[ 0.07348098 -0.00939143 -0.02649076  0.03276831]\n",
      " [ 0.07348098 -0.00939143 -0.02649076  0.03276831]], shape=(2, 4), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "x =  tf.ones((2,2))\n",
    "linear_layer = Linear(4,2)\n",
    "y = linear_layer(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert linear_layer.weights == [linear_layer.w, linear_layer.b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'Variable:0' shape=(2, 4) dtype=float32, numpy=\n",
       " array([[ 6.2038124e-02,  3.3474594e-06, -4.8606031e-02,  4.1684832e-02],\n",
       "        [ 1.1442853e-02, -9.3947789e-03,  2.2115273e-02, -8.9165196e-03]],\n",
       "       dtype=float32)>,\n",
       " <tf.Variable 'Variable:0' shape=(4,) dtype=float32, numpy=array([0., 0., 0., 0.], dtype=float32)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_layer.weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A layer can have non-trainable weights. Meaning the weights are not taken into account during backpropagation when we are training the layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sum(tf.keras.layers.Layer):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.total = tf.Variable(initial_value=tf.zeros((input_dim, )), \n",
    "                                 trainable= False)\n",
    "    def call(self, inputs):\n",
    "        self.total.assign_add(tf.reduce_sum(inputs, axis=0))\n",
    "        return self.total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'Variable:0' shape=(2,) dtype=float32, numpy=array([6., 6.], dtype=float32)>\n"
     ]
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "my_sum = Sum(2)\n",
    "y = my_sum(x)\n",
    "y = my_sum(x)\n",
    "y = my_sum(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self, units=64):\n",
    "        super().__init__()\n",
    "        self.units = units\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        self.w = self.add_weight(shape=(input_shape[-1], self.units), \n",
    "                                 initializer='random_normal', \n",
    "                                 trainable=True)\n",
    "        self.b = self.add_weight(shape=(self.units,),\n",
    "                                 initializer='random_normal',\n",
    "                                trainable=True)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        print(self.b)\n",
    "        return tf.matmul(inputs, self.w) + self.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'linear_1/Variable:0' shape=(10,) dtype=float32, numpy=\n",
      "array([-0.08670553,  0.00480035, -0.05248139,  0.00826279, -0.09604456,\n",
      "        0.00295295,  0.03094005, -0.02024541, -0.08639894, -0.08661783],\n",
      "      dtype=float32)>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=3493, shape=(2, 10), dtype=float32, numpy=\n",
       "array([[-0.19240245,  0.03941366, -0.02431128, -0.02553701, -0.1108198 ,\n",
       "         0.09344528, -0.03169997, -0.03902419, -0.06124165, -0.14791548],\n",
       "       [-0.19240245,  0.03941366, -0.02431128, -0.02553701, -0.1108198 ,\n",
       "         0.09344528, -0.03169997, -0.03902419, -0.06124165, -0.14791548]],\n",
       "      dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = tf.ones((2,2))\n",
    "linear_layer = Linear(10)\n",
    "y = linear_layer(x)\n",
    "y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers are recursively composable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPBlock(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear1 = Linear(32)\n",
    "        self.linear2 = Linear(32)\n",
    "        self.linear3 = Linear(32)\n",
    "        self.linear4 = Linear(1)\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        x = self.linear1(inputs)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear2(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        x = self.linear3(x)\n",
    "        x = tf.nn.relu(x)\n",
    "        return self.linear4(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<tf.Variable 'mlp_block/linear_2/Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 0.03481292, -0.00940918,  0.02307169, -0.00797744, -0.03739671,\n",
      "        0.11979245, -0.00796023,  0.07728885, -0.1614969 ,  0.02622102,\n",
      "        0.02817338, -0.05258849, -0.00651452,  0.02734741, -0.01881116,\n",
      "        0.0220915 ,  0.0023178 , -0.05221058,  0.00597975,  0.02553415,\n",
      "        0.07105091,  0.03704478, -0.00628842,  0.02715683, -0.0654978 ,\n",
      "        0.06027733, -0.00441276, -0.03020672,  0.06210121, -0.00742736,\n",
      "       -0.00546706, -0.00863805], dtype=float32)>\n",
      "<tf.Variable 'mlp_block/linear_3/Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([ 0.06679098, -0.07779744,  0.00693042, -0.0392703 ,  0.02995011,\n",
      "       -0.02553695,  0.10237887,  0.01981111, -0.09270662,  0.02814372,\n",
      "       -0.01603961,  0.08201616,  0.01768069, -0.055011  , -0.05020701,\n",
      "       -0.02050759, -0.02410841, -0.01638143,  0.03707058,  0.07861806,\n",
      "       -0.02848534,  0.01321686, -0.02443574, -0.07694656,  0.00072243,\n",
      "        0.06605083,  0.10023194,  0.08031628,  0.02799926, -0.03124868,\n",
      "       -0.06473979,  0.03582655], dtype=float32)>\n",
      "<tf.Variable 'mlp_block/linear_4/Variable:0' shape=(32,) dtype=float32, numpy=\n",
      "array([-0.01754539, -0.03704799, -0.01091773, -0.01396134, -0.02541072,\n",
      "        0.02991871, -0.08033863, -0.02876926,  0.02637352,  0.00973751,\n",
      "       -0.02616416,  0.00429343, -0.00576089,  0.03542333, -0.01661202,\n",
      "        0.0944298 ,  0.07415918,  0.03470778, -0.06906741, -0.01412433,\n",
      "        0.03773317, -0.0584932 , -0.0614985 , -0.03665721, -0.00869526,\n",
      "       -0.0719277 , -0.00994887,  0.0621469 , -0.09678892,  0.01823409,\n",
      "       -0.04011325,  0.01458411], dtype=float32)>\n",
      "<tf.Variable 'mlp_block/linear_5/Variable:0' shape=(1,) dtype=float32, numpy=array([0.02325526], dtype=float32)>\n",
      "Weights:  8\n",
      "trainable weights:  8\n"
     ]
    }
   ],
   "source": [
    "mlp = MLPBlock()\n",
    "y = mlp(tf.ones(shape=(3,64)))\n",
    "print(\"Weights: \", len(mlp.weights))\n",
    "print(\"trainable weights: \", len(mlp.trainable_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom ConvLayer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvLayer(tf.keras.layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.w_conv1 = tf.Variable(tf.random.normal([5,5,1,32],\n",
    "                                                       stddev=0.1))\n",
    "        self.b_conv1 = tf.Variable(tf.constant(0.1, shape=[32]))\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        self.convolve1 = tf.nn.conv2d(inputs, self.w_conv1, \n",
    "                                     strides=[1,1,1,1], padding='SAME') + self.b_conv1\n",
    "        self.h_conv1 = tf.nn.relu(self.convolve1)\n",
    "        self.conv1 = tf.nn.max_pool(self.h_conv1, ksize=[1,2,2,1], strides=[1,2,2,1], padding='SAME')\n",
    "        return self.conv1.shape\n",
    "    \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer = ConvLayer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.ones((1, 28,28,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 14, 14, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "layer(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
